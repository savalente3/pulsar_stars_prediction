{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0666a1c49e4e5fba6025a80cf74ffc6eba601b68aad856283517ff259570196a0",
   "display_name": "Python 3.9.4 64-bit ('venv': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "b924adaafa3766dc1e2de79a4075e03b8518570ff546375d9f2e6b678978f4bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "'''Predicting Pulsar Stars'''\n",
    "#binary classification algorithm\n",
    "\n",
    "#Load Models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, train_test_split, RepeatedStratifiedKFold\n",
    "\n",
    "#shows visualization in line -> replaces plt.show\n",
    "%matplotlib inline"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING DATA\n",
    "\n",
    "data = pd.read_csv('Datasets/pulsar_star_dataset/pulsar_data_train.csv')\n"
   ]
  },
  {
   "source": [
    "'''DATA TREATMENT'''\n",
    "#describes testing set shape, null values anda data info\n",
    "  \n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"Dataset's Shape: \", data.shape)\n",
    "\n",
    "print(\"--------------------------------------------------- \")\n",
    "print(\"Null Values: \")\n",
    "print(data.isna().sum())\n",
    "\n",
    "print(\"--------------------------------------------------- \")\n",
    "print(\"Data Info: \")\n",
    "print(data.info())\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As all columns are relevant datapoints, none are droped \n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demonstrates Nan values within the dataset\n",
    "#white strips represents Nan values in a column\n",
    "\n",
    "msno.matrix(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drops rows with Nan values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "#demonstrates the data shape to confirm the treated data has at least 1000 entrie\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"Dataset's Shape: \", data.shape)\n",
    "\n",
    "print(\"--------------------------------------------------- \")\n",
    "print(\"Null Values: \")\n",
    "print(data.isna().sum())\n",
    "\n",
    "print(\"--------------------------------------------------- \")\n",
    "print(\"Data Info: \")\n",
    "print(data.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As all columns are relevant datapoints, none are droped \n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demonstrates Nan values within the dataset\n",
    "#lack of white strips represents the lack of Nan values\n",
    "\n",
    "msno.matrix(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As target_class is already binary, theres not need to transform it into categortical values\n",
    "#gives general infor about the data\n",
    "\n",
    "data.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantifies how many pulsar stars exist in the training set\n",
    "sns.countplot(x=data['target_class'],label=\"pulsar_star\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE SELECTION\n",
    "#finds correlations between data\n",
    "\n",
    "data_corr = data.corr()\n",
    "data_corr.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatmap analyses the feature correlation\n",
    "def heatmap(data):\n",
    "    plt.figure()\n",
    "    sns.heatmap(data_corr)\n",
    "\n",
    "heatmap(data_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cheecks and eliminates one of the features that have a correlation of over .85\n",
    "corr_columns = np.full((data_corr.shape[0],), True, dtype=bool)\n",
    "\n",
    "for i in range(data_corr.shape[0]):\n",
    "    for j in range(i+1, data_corr.shape[0]):\n",
    "        if data_corr.iloc[i,j] >= 0.85:\n",
    "            if corr_columns[j]:\n",
    "                corr_columns[j] = False\n",
    "\n",
    "selected_columns = data.columns[corr_columns]\n",
    "data_f = data[selected_columns]\n",
    "\n",
    "\n",
    "#To check there are no correlation between features with values over .85\n",
    "data_corr = data_f.corr()\n",
    "heatmap(data_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA PLOT\n",
    "def PCA_Plot(data):\n",
    "\n",
    "    #defining variables\n",
    "    data_X = data.iloc[:,0:-1].values\n",
    "    data_y = pd.DataFrame(data_f.iloc[:,-1].values, columns=['target_class'])\n",
    "\n",
    "\n",
    "    #Scale X values to remove mean and improve accuracy\n",
    "    X_std = StandardScaler().fit_transform(data_X)\n",
    "\n",
    "\n",
    "    #PCA\n",
    "    #Tripathi, A. (2019) A Complete Guide to Principal Component Analysis â€“ PCA in Machine Learning, Data Science Duniya. \n",
    "    #Available at: https://ashutoshtripathi.com/2019/07/11/a-complete-guide-to-principal-component-analysis-pca-in-machine-learning/ (Accessed: 27 April 2021).\n",
    "    pca = PCA(n_components=2) \n",
    "    principalComponents = pca.fit_transform(X_std) \n",
    "    principalDf = pd.DataFrame(data=principalComponents , columns = ['principal component 1', 'principal component 2'])\n",
    "    finalDf = pd.concat([principalDf, data_y], axis = 1)\n",
    "\n",
    "\n",
    "    #PCA_Plot\n",
    "    plt.figure()\n",
    "    plt.xlabel('Principal component 1')\n",
    "    plt.ylabel('Principal component 2')\n",
    "    plt.suptitle(\"Pulsar Stars Prediction\")\n",
    "    labels = [\"Not a Pulsar Star\",\"Pulsar Star\"]\n",
    "    scatter = plt.scatter(data=finalDf, x=\"principal component 1\", y=\"principal component 2\", c=\"target_class\",cmap='Spectral', label = labels)\n",
    "    plt.legend(handles=scatter.legend_elements()[0], labels=labels)\n",
    "\n",
    "\n",
    "PCA_Plot(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_set(data):\n",
    "    \n",
    "    #defining variables\n",
    "    data_X = data.iloc[:,0:-1].values\n",
    "    data_y = data.iloc[:,-1].values\n",
    "\n",
    "    #scale dataf_X values to remove mean and improve accuracy\n",
    "    #not applying scaling on y_train and y_test since their values are already 0 and 1.\n",
    "    X_scaler = StandardScaler().fit_transform(data_X)\n",
    "\n",
    "    #defining training and testing variables\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaler, data_y, test_size=0.3, random_state=0)\n",
    "\n",
    "    return (X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acuracy results from models\n",
    "def accuracy_results(model, X_train, y_train, X_test, y_test, y_test_pred):\n",
    "\n",
    "    #evaluate a score by cross-validation\n",
    "    scores = cross_val_score(model, X_test, y_test, cv=5, scoring='accuracy')\n",
    "\n",
    "    print(\"--------------------------------------------------- \")\n",
    "    print(\"Model training accuracy: \", round(model.score(X_train, y_train), 5))\n",
    "    print(\"Model testing accuracy: \", round(model.score(X_test, y_test), 5))\n",
    "    print(\"Maximun Scaled accuracy: \", round(accuracy_score(y_test, y_test_pred), 5))\n",
    "    print(\"Cross Validation Accuracy: \", round(scores.mean(), 5))\n",
    "    print(\"--------------------------------------------------- \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEARNING MODELS\n",
    "\n",
    "'''Logistic Regression'''\n",
    "def logistic_reg(data):\n",
    "\n",
    "    #gets vars dataf_X, dataf_y, X_train, X_test, y_train, y_test with data\n",
    "    X_train, X_test, y_train, y_test = train_test_set(data)\n",
    "\n",
    "\n",
    "    #training the model\n",
    "    model_LR = LogisticRegression()\n",
    "    model_LR = model_LR.fit(X_train, y_train)\n",
    "    X_train_pred = model_LR.predict(X_train)\n",
    "    y_test_pred = model_LR.predict(X_test)\n",
    "\n",
    "\n",
    "    #Confusion Matrix Normalized \n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred, normalize='all')\n",
    "\n",
    "    #prints accuracies results and scores\n",
    "    accuracy_results(model_LR, X_train, y_train, X_test, y_test, y_test_pred)\n",
    "\n",
    "    #heatmap plot of CONFUSION MATRIX\n",
    "    labels = ['NOT_pulsar_star', 'pulsar_star']\n",
    "    heatmap = sns.heatmap(conf_matrix, annot=True)\n",
    "    heatmap.set_xticklabels(labels)\n",
    "    heatmap.set_yticklabels(labels)\n",
    "    heatmap.set(ylabel=\"Real values\", xlabel=\"Predicted values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression with original data\n",
    "logistic_reg(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression with data with feature scalling (data_f)\n",
    "logistic_reg(data_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression with grid search\n",
    "def logistic_reg_grid_search(data):\n",
    "\n",
    "    #gets vars dataf_X, dataf_y, X_train, X_test, y_train, y_test with data\n",
    "    X_train, X_test, y_train, y_test = train_test_set(data)\n",
    "\n",
    "    \n",
    "    #GRID SEARCH\n",
    "\n",
    "    #Defining parameters\n",
    "    #defining solvers optimises the algorithm\n",
    "    solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "    #penalises the hyperparameter\n",
    "    penalty = ['l1', 'l2']\n",
    "    #strengh of penalty\n",
    "    c_param = [100, 10, 1.0, 0.1, 0.01, 0.001]\n",
    "\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    param_GS = dict(solver=solvers, penalty=penalty, C=c_param)\n",
    "    model_GS = GridSearchCV(estimator=LogisticRegression(), param_grid=param_GS, n_jobs=-1, cv=cv, scoring='accuracy', error_score=0)\n",
    "    \n",
    "    #fitting the model\n",
    "    model_GS = model_GS.fit(X_train, y_train)\n",
    "    X_train_pred = model_GS.predict(X_train)\n",
    "    y_test_pred = model_GS.predict(X_test)\n",
    "\n",
    "    #Confusion Matrix Normalized \n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred, normalize='all')\n",
    "\n",
    "    #prints accuracies results and scores\n",
    "    print(\"--------------------------------------------------- \")\n",
    "    print(\"Best Accuracy: \", round(model_GS.best_score_, 5))\n",
    "    print(\"Best hyperparameters: \", model_GS.best_params_)\n",
    "    accuracy_results(model_GS, X_train, y_train, X_test, y_test, y_test_pred)\n",
    "\n",
    "    #heatmap plot of CONFUSION MATRIX\n",
    "    labels = ['NOT_pulsar_star', 'pulsar_star']\n",
    "    heatmap = sns.heatmap(conf_matrix, annot=True)\n",
    "    heatmap.set_xticklabels(labels)\n",
    "    heatmap.set_yticklabels(labels)\n",
    "    heatmap.set(ylabel=\"Real values\", xlabel=\"Predicted values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression with original data and grid search\n",
    "logistic_reg_grid_search(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression with feature selection data and grid search\n",
    "logistic_reg_grid_search(data_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression with random search\n",
    "def logistic_reg_random_search(data):\n",
    "\n",
    "    #gets vars dataf_X, dataf_y, X_train, X_test, y_train, y_test with data\n",
    "    X_train, X_test, y_train, y_test = train_test_set(data)\n",
    "\n",
    "    \n",
    "    #RANDOM SEARCH\n",
    "\n",
    "    #Defining parameters\n",
    "    solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "    penalty = ['l1', 'l2']\n",
    "    c_param = [100, 10, 1.0, 0.1, 0.01, 0.001]\n",
    "\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    param_RS = dict(solver=solvers, penalty=penalty, C=c_param)\n",
    "    \n",
    "    model_RS = RandomizedSearchCV(LogisticRegression(), param_RS, n_jobs=-1, cv=cv, scoring='accuracy', error_score=0)\n",
    "    \n",
    "    #fitting the model\n",
    "    model_RS = model_RS.fit(X_train, y_train)\n",
    "    X_train_pred = model_RS.predict(X_train)\n",
    "    y_test_pred = model_RS.predict(X_test)\n",
    "\n",
    "    #Confusion Matrix Normalized \n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred, normalize='all')\n",
    "\n",
    "    #prints accuracies results and scores\n",
    "    print(\"--------------------------------------------------- \")\n",
    "    print(\"Best Accuracy: \", round(model_RS.best_score_, 5))\n",
    "    print(\"Best hyperparameters: \", model_RS.best_params_)\n",
    "    accuracy_results(model_RS, X_train, y_train, X_test, y_test, y_test_pred)\n",
    "\n",
    "    #heatmap plot of CONFUSION MATRIX\n",
    "    labels = ['NOT_pulsar_star', 'pulsar_star']\n",
    "    heatmap = sns.heatmap(conf_matrix, annot=True)\n",
    "    heatmap.set_xticklabels(labels)\n",
    "    heatmap.set_yticklabels(labels)\n",
    "    heatmap.set(ylabel=\"Real values\", xlabel=\"Predicted values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression with original data and random search\n",
    "logistic_reg_random_search(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression with feature selection data and random search\n",
    "logistic_reg_random_search(data_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''KNeighbours'''\n",
    "def KN_Neighbors(data):\n",
    "\n",
    "    #gets vars dataf_X, dataf_y, X_train, X_test, y_train, y_test with data\n",
    "    X_train, X_test, y_train, y_test = train_test_set(data)\n",
    "\n",
    "\n",
    "    #training the model\n",
    "    model_KN = KNeighborsClassifier()\n",
    "    model_KN = model_KN.fit(X_train, y_train)\n",
    "    X_train_pred = model_KN.predict(X_train)\n",
    "    y_test_pred = model_KN.predict(X_test)\n",
    "\n",
    "\n",
    "    #Confusion Matrix Normalized \n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred, normalize='all')\n",
    "\n",
    "\n",
    "    #prints accuracies results and scores\n",
    "    accuracy_results(model_KN, X_train, y_train, X_test, y_test, y_test_pred)\n",
    "\n",
    "\n",
    "    #heatmap plot of CONFUSION MATRIX\n",
    "    labels = ['NOT_pulsar_star', 'pulsar_star']\n",
    "    heatmap = sns.heatmap(conf_matrix, annot=True)\n",
    "    heatmap.set_xticklabels(labels)\n",
    "    heatmap.set_yticklabels(labels)\n",
    "    heatmap.set(ylabel=\"Real values\", xlabel=\"Predicted values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNNeighbors with original data\n",
    "KN_Neighbors(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNNeighbors with feature selection data\n",
    "KN_Neighbors(data_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNNeighbors with grid search\n",
    "def KN_Neighbors_grid_search(data):\n",
    "\n",
    "    #gets vars dataf_X, dataf_y, X_train, X_test, y_train, y_test with data\n",
    "    X_train, X_test, y_train, y_test = train_test_set(data)\n",
    "\n",
    "    \n",
    "    #GRID SEARCH\n",
    "\n",
    "    #Defining parameters\n",
    "    n_neighbors = range(1, 31)\n",
    "    #checks uniform or distance\n",
    "    weights = ['uniform', 'distance']\n",
    "    #metrics\n",
    "    metric = ['euclidian', 'manhattan', 'minkowski']\n",
    "\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    param_GS = dict(n_neighbors=n_neighbors, weights=weights, metric=metric)\n",
    "    model_GS = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param_GS, n_jobs=-1, cv=cv, scoring='accuracy', error_score=0)\n",
    "    \n",
    "    #fitting the model\n",
    "    model_GS = model_GS.fit(X_train, y_train)\n",
    "    X_train_pred = model_GS.predict(X_train)\n",
    "    y_test_pred = model_GS.predict(X_test)\n",
    "\n",
    "    #Confusion Matrix Normalized \n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred, normalize='all')\n",
    "\n",
    "    #prints accuracies results and scores\n",
    "    print(\"--------------------------------------------------- \")\n",
    "    print(\"Best Accuracy: \", round(model_GS.best_score_, 5))\n",
    "    print(\"Best hyperparameters: \", model_GS.best_params_)\n",
    "    accuracy_results(model_GS, X_train, y_train, X_test, y_test, y_test_pred)\n",
    "\n",
    "    #heatmap plot of CONFUSION MATRIX\n",
    "    labels = ['NOT_pulsar_star', 'pulsar_star']\n",
    "    heatmap = sns.heatmap(conf_matrix, annot=True)\n",
    "    heatmap.set_xticklabels(labels)\n",
    "    heatmap.set_yticklabels(labels)\n",
    "    heatmap.set(ylabel=\"Real values\", xlabel=\"Predicted values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNNeighbors with original data with grid search\n",
    "KN_Neighbors_grid_search(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNNeighbors with feature selection data with grid search\n",
    "KN_Neighbors_grid_search(data_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNNeighbors with random search\n",
    "def KN_Neighbors_random_search(data):\n",
    "\n",
    "    #gets vars dataf_X, dataf_y, X_train, X_test, y_train, y_test with data\n",
    "    X_train, X_test, y_train, y_test = train_test_set(data)\n",
    "\n",
    "\n",
    "    #Defining parameters\n",
    "    n_neighbors = range(1, 31, 2)\n",
    "    #checks uniform or distance\n",
    "    weights = ['uniform', 'distance']\n",
    "    #metrics\n",
    "    metric = ['euclidian', 'manhattan', 'minkowski']\n",
    "\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    param_RS = dict(n_neighbors=n_neighbors, weights=weights, metric=metric)\n",
    "    model_RS = RandomizedSearchCV(KNeighborsClassifier(), param_RS, n_jobs=-1, cv=cv, scoring='accuracy', error_score=0)\n",
    "    \n",
    "    #fitting the model\n",
    "    model_RS = model_RS.fit(X_train, y_train)\n",
    "    X_train_pred = model_RS.predict(X_train)\n",
    "    y_test_pred = model_RS.predict(X_test)\n",
    "\n",
    "    #Confusion Matrix Normalized \n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred, normalize='all')\n",
    "\n",
    "    #prints accuracies results and scores\n",
    "    print(\"--------------------------------------------------- \")\n",
    "    print(\"Best Accuracy: \", round(model_RS.best_score_, 5))\n",
    "    print(\"Best hyperparameters: \", model_RS.best_params_)\n",
    "    accuracy_results(model_RS, X_train, y_train, X_test, y_test, y_test_pred)\n",
    "\n",
    "    #heatmap plot of CONFUSION MATRIX\n",
    "    labels = ['NOT_pulsar_star', 'pulsar_star']\n",
    "    heatmap = sns.heatmap(conf_matrix, annot=True)\n",
    "    heatmap.set_xticklabels(labels)\n",
    "    heatmap.set_yticklabels(labels)\n",
    "    heatmap.set(ylabel=\"Real values\", xlabel=\"Predicted values\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNNeighbors with original data with random search\n",
    "KN_Neighbors_random_search(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNNeighbors with feature selection data and random search\n",
    "KN_Neighbors_random_search(data_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}